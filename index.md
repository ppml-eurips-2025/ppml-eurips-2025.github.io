---
layout: single
title: "PPML @ EurIPS 2025"
permalink: /
classes: wide
header:
    overlay_image: "assets/images/cop2.JPG"
    overlay_filter: 0.4
    overlay_color: "#000"
    caption: "Copenhagen"

feature_row:
  - image_path: /assets/images/amartya_sq.png
    alt: "Amartya Sanyal"
    excerpt: "**[Amartya Sanyal](https://amartya18x.github.io/)**<br><small>University Of Copenhagen</small>"
  - image_path: /assets/images/edwige_sq.jpeg
    alt: "Edwige Cyffers"
    excerpt: "**[Edwige Cyffers](https://perso.ens-lyon.fr/edwige.cyffers/)**<br><small>ISTA</small>"
  - image_path: /assets/images/Cummings037RT-min-cropped_0_sq.jpg
    alt: "Rachel Cummings"
    excerpt: "**[Rachel Cummings](https://rachelcummings.com/)**<br><small>Columbia University</small>"
  - image_path: /assets/images/chl__sq.jpg
    alt: "Christoph Lampert"
    excerpt: "**[Christoph Lampert](https://cvml.ista.ac.at/)**<br><small>ISTA</small>"
  - image_path: /assets/images/nikita_poster_sq.jpg
    alt: "Nikita Kalinin"
    excerpt: "**[Nikita Kalinin](https://npkalinin.github.io/)**<br><small>ISTA</small>"
  - image_path: /assets/images/peter_sq.png
    alt: "Peter Kairouz"
    excerpt: "**[Peter Kairouz](https://kairouzp.github.io/)**<br><small>Google</small>"

feature_row_bis:
  - image_path: /assets/images/aure_sq.jpg
    alt: "Aurélien Bellet"
    excerpt: "**[Aurélien Bellet](https://researchers.lille.inria.fr/abellet//)**<br><small>Inria</small>"
  - image_path: /assets/images/tamalika_sq.jpg
    alt: "Tamalika Mukherjee"
    excerpt: "**[Tamalika Mukherjee](https://tamalikamukherjee.com/)**<br><small>Max Planck Institute for Security and Privacy</small>"
  - image_path: /assets/images/antti_sq.jpg
    alt: "Antti Honkela"
    excerpt: "**[Antti Honkela](https://www.cs.helsinki.fi/u/ahonkela/)**<br><small>University of Helsinki</small>"
  - image_path: /assets/images/rasmus_sq.jpg
    alt: "Rasmus Pagh"
    excerpt: "**[Rasmus Pagh](https://rasmuspagh.net/)**<br><small>University of Copenhagen</small>"
  - image_path: /assets/images/catuscia_sq.jpg
    alt: "Catuscia Palamidessi"
    excerpt: "**[Catuscia Palamidessi](https://www.lix.polytechnique.fr/%7Ecatuscia/)**<br><small>Inria</small>"
  - image_path: /assets/images/sahra_sq.jpg
    alt: "Sahra Ghalebikesabi"
    excerpt: "**[Sahra Ghalebikesabi](https://sghalebikesabi.github.io/)**<br><small>ex-Google DeepMind</small>"

---


## About {#about}

Welcome to the **Privacy-Preserving Machine Learning Workshop at [EurIPS](http://eurips.cc) 2025**!

The success of machine learning depends on access to large amounts of training data, which often contains sensitive information. This raises issues of legality, competitiveness, and privacy when data is exposed. Neural networks are known to be vulnerable to privacy attacks, a concern that has recently become more visible with large language models (LLMs), where attacks can be carried out directly through prompting. Differential privacy, the gold standard for privacy-preserving learning, has improved in terms of privacy–utility trade-offs thanks to new trust models and algorithms. However, there are still many open questions on how to bridge the gap between attacks and defenses, from developing auditing methods and more effective attacks to the growing interest in machine unlearning.

**Which models best reflect real-world scenarios? How can methods scale to deep learning and foundation models? How are unlearning, auditing, and privacy-preserving machine learning connected, and how can these lines of work be brought together?**

This workshop will bring together researchers from academia and industry working on differential privacy, machine unlearning, privacy auditing, privacy attacks, and related topics.


## Call for Papers {#cfp}

We invite submissions to the **Privacy-Preserving Machine Learning Workshop at EurIPS 2025**.  
We welcome both novel contributions and in-progress work with diverse viewpoints.  


| Important Dates                  | Date (AoE)           |
|-------------------------------------------------|----------------------|
| Paper submission     | **October 17, 2025** |
| Accept/Reject notification            | **October 31, 2025** |
| Workshop                                  | **December 7, 2025** |

All accepted papers can be presented as posters. Posterboards allow for **A0 portrait** or A1 landscape. (Please use A0 portrait if possible).


### Topics of Interest
- Efficient methods for privacy-preserving machine learning  
- Trust models for privacy, including federated learning and data minimization  
- Privacy at inference and privacy for agents interaction and for large language models (fine-tuning, test-time training)  
- Privacy-preserving data generation  
- Differential privacy theory
- Threat models and privacy attacks  
- Auditing methods and interpretation of privacy guarantees  
- Machine unlearning, certifiable machine unlearning, and new unlearning algorithms  
- Relationship between privacy and other issues related to Trustworthy Machine Learning  

### Submission Guidelines
- **Format:** up to **5 pages**, excluding references  
- **Style:** [NeurIPS 2025 template](https://neurips.cc/Conferences/2025/CallForPapers)  
- **Anonymization:** required (double-blind review)  
- **Submission site:** via **OpenReview**: [https://openreview.net/group?id=EurIPS.cc/2025/Workshop/PPML]( https://openreview.net/group?id=EurIPS.cc/2025/Workshop/PPML)


This workshop is **non-archival and will not have official proceedings**. Workshop submissions can be submitted to other venues. We welcome ongoing and unpublished work, including papers that are under review at the time of submission. We do not accept submissions that have already been accepted for publication in other venues with archival proceedings. The titles of accepted papers will be published on the website.

We are looking for reviewers to help ensure a fair and constructive review process.  
Each reviewer will be asked to review at most **3 papers**.  


---
## Invited Speakers {#speakers}

{% include feature_row id="feature_row_bis" %}


## Program {#program}

The workshop will be at ITU, Auditorium 0. There are 180 seats and food for everyone! We have six invited talks and 3 shorter contributed talks (CT).

| Time | Program |
| - | - |
| 08:50 - 09:00 | Opening |
| 09:00 - 10:30 | **Interpreting Privacy Guarantees**|
| |*On quantifying and communicating privacy* Antti Honkela |
| | *A Causal Perspective on Membership Inference Attacks* Aurélien Bellet |
| | CT: *Privacy Amplification Persists Under Unlimited Data Release* Clément Pierquin |
| 10:30 - 11:00 | Coffee Break (*please install the posters during the break*) |
| 11:00 - 12:30 | **Deep Learning and agents** |
| | *Operationalising Contextual Integrity in Privacy-conscious agents* Sahra Ghalebikesabi|
| | CT: *Efficient and Scalable Implementation of Differentially Private Deep Learning without Shortcuts* Sebastian Rodriguez Beltran |
| | First Poster session |
| 12:30 - 13:30 | Lunch |
| 13:30 - 15:00 | **Making DP efficient** |
| | *Privacy Under Memory Constraints* Tamalika Mukherjee |
| | *How many random bits are needed for differential privacy?* Rasmus Pagh |
| | CT: *Better Rates for Private Linear Regression in the Proportional Regime via Aggressive Clipping* Inbar Seroussi|
| 15:00 - 15:30 | Coffee Break |
| 15:30 - 17:00 | **Federated Learning** |
| | *Privacy-preserving Federated Histogram Estimation: Local Differential Privacy Strikes Back* Catuscia Palamidessi |
| | Second Poster session and Awards! |

## Accepted papers

- "_Privacy Leakage via Output Label Space and Differentially Private Continual Learning_", Marlon Tobaben, Talal Alrawajfeh, Marcus Klasson, Mikko A. Heikkilä, Arno Solin, Antti Honkela
- "_Privacy Amplification Persists Under Unlimited Data Release_", Clément Pierquin, Aurélien Bellet, Marc Tommasi, Matthieu Boussard
- "_Subgroup-Level Membership Inference Risks in Synthetic RNA-seq_", Hakime Öztürk, Oliver Stegle
- "_Privacy Preserving Diffusion Models for Mixed-Type Tabular Data Generation_", Timur Sattarov, Marco Schreyer, Damian Borth
- "_A New Sensitivity Bound on Sliced Wasserstein Losses for Private Machine Learning_", David Rodríguez-Vítores, Clément Lalanne, Jean-Michel Loubes
- "_Just a Simple Transformation is Enough for Data Protection in Split Learning_", Andrei Semenov, Philip Zmushko, Alexander Pichugin, Aleksandr Beznosikov
- "_Mitigating Disparate Impact of Differentially Private Learning through Bounded Adaptive Clipping_", Linzh Zhao, Aki Rehn, Mikko A. Heikkilä, Razane Tajeddine, Antti Honkela
- "_Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization_", Aurélien Bellet, Edwige Cyffers, Davide Frey, Romaric Gaudel, Dimitri Lerévérend, Francois Taiani
- "_How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding_", Mathieu Dufour, Andrew B. Duncan
- "_A Law of Data Reconstruction for Random Features (and Beyond)_", Simone Bombari, Leonardo Iurada, Tatiana Tommasi, Marco Mondelli
- "_Balancing Fairness and Privacy in DP-SGD with Subsampling and Clipping_", Max Cairney-Leeming, Christoph H. Lampert, Amartya Sanyal
- "_Communication-efficient publication of sparse vectors under DP via Poisson private representation_", Quentin Hillebrand, Vorapong Suppakitpaisarn, Tetsuo Shibuya
- "_An Interactive Framework for Finding the Optimal Trade-off in Differential Privacy_", Yaohong Yang, Aki Rehn, Sammie Katt, Antti Honkela, Samuel Kaski
- "_Efficient and Scalable Implementation of Differentially Private Deep Learning without Shortcuts_", Sebastian Rodriguez Beltran, Marlon Tobaben, Joonas Jälkö, Niki Andreas Loppi, Antti Honkela
- "_iDP-ULDP: Achieving Tight User-Level DP with Heterogeneous Numbers of Records per User_", Johannes Kaiser, Jakob Eigenmann, Daniel Rueckert, Georgios Kaissis
- "_DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning_", Mihaela Hudişteanu, Nikita Kalinin, Edwige Cyffers
- "_Differentially Private and Federated Structure Learning in Bayesian Networks_", Ghita Fassy El Fehri, Aurélien Bellet, Philippe Bastien
- "_Private Rate-Constrained Optimization with Applications to Fair Learning_", Mohammad Yaghini, Tudor Cebere, Michael Menart, Aurélien Bellet, Nicolas Papernot
- "_Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy_", Bogdan Kulynych, Juan Felipe Gomez, Jamie Hayes, Borja Balle, Flavio Calmon, Georgios Kaissis, Jean Louis Raisaro
- "_Better Rates for Private Linear Regression in the Proportional Regime via Aggressive Clipping_", Simone Bombari, Inbar Seroussi, Marco Mondelli
- "_On Optimal Hyperparameters for Differentially Private Deep Transfer Learning_", Aki Rehn, Linzh Zhao, Mikko A. Heikkilä, Antti Honkela
- "_Sequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning_", Dolgova Polina, Sebastian U Stich
- "_Model Agnostic Differentially Private Causal Inference_", Christian Janos Lebeda, Mathieu Even, Aurélien Bellet, Julie Josse
- "_Beyond Membership: Limitations of Add/Remove Adjacency in Differential Privacy_", Gauri Pradhan, Joonas Jälkö, Santiago Zanella-Beguelin, Antti Honkela

---

## Organizers {#organizers}


{% include feature_row id="feature_row" %}

## Reviewers

We thank all the reviewers for their work.
- Bogdan Kulynych
- Carolin Heinzler
- Christian Janos Lebeda
- Christoph H. Lampert
- Clément Lalanne
- Clément Pierquin
- Edwige Cyffers
- Erchi Wang
- Jan Schuchardt
- Joel Daniel Andersson
- Kostadin Cvejoski
- Luca Corbucci
- Lukas Retschmeier
- Marlon Tobaben
- Mathieu Dagréou
- Mina Basirat
- Nikita Kalinin
- Quentin Hillebrand
- Renaud Gaucher
- Romaric Gaudel
- Şeyma Selcan Mağara
- Simone Bombari
- Vasilis Siomos

## Sponsor

PPML@EurIPS is sponsored by [BILAI](https://bilateral-ai.net/).
{% include figure popup=true image_path="/assets/images/Bilai.png" alt="Bilai Logo" caption="" %}


## Contact {#contact}
Questions? Email us at **[ppml.eurips@gmail.com](mailto:ppml.eurips@gmail.com)**

