---
layout: single
title: "PPML @ EurIPS 2025"
permalink: /
classes: wide
header:
    overlay_image: "assets/images/cop2.JPG"
    overlay_filter: 0.4
    overlay_color: "#000"
    caption: "Copenhagen"

feature_row:
  - image_path: /assets/images/amartya_sq.png
    alt: "Amartya Sanyal"
    excerpt: "**[Amartya Sanyal](https://amartya18x.github.io/)**<br><small>University Of Copenhagen</small>"
  - image_path: /assets/images/edwige_sq.jpeg
    alt: "Edwige Cyffers"
    excerpt: "**[Edwige Cyffers](https://perso.ens-lyon.fr/edwige.cyffers/)**<br><small>ISTA</small>"
  - image_path: /assets/images/Cummings037RT-min-cropped_0_sq.jpg
    alt: "Rachel Cummings"
    excerpt: "**[Rachel Cummings](https://rachelcummings.com/)**<br><small>Columbia University</small>"
  - image_path: /assets/images/chl__sq.jpg
    alt: "Christoph Lampert"
    excerpt: "**[Christoph Lampert](https://cvml.ista.ac.at/)**<br><small>ISTA</small>"
  - image_path: /assets/images/nikita_poster_sq.jpg
    alt: "Nikita Kalinin"
    excerpt: "**[Nikita Kalinin](https://npkalinin.github.io/)**<br><small>ISTA</small>"
  - image_path: /assets/images/peter_sq.png
    alt: "Peter Kairouz"
    excerpt: "**[Peter Kairouz](https://kairouzp.github.io/)**<br><small>Google</small>"

feature_row_bis:
  - image_path: /assets/images/aure_sq.jpg
    alt: "Aurélien Bellet"
    excerpt: "**[Arélien Bellet](https://researchers.lille.inria.fr/abellet//)**<br><small>Inria</small>"
  - image_path: /assets/images/tamalika_sq.jpg
    alt: "Tamalika Mukherjee"
    excerpt: "**[Tamalika Mukherjee](https://tamalikamukherjee.com/)**<br><small>Max Planck Institute for Security and Privacy</small>"
  - image_path: /assets/images/antti_sq.jpg
    alt: "Antti Honkela"
    excerpt: "**[Antti Honkela](https://www.cs.helsinki.fi/u/ahonkela/)**<br><small>University of Helsinki</small>"


---


## About {#about}

Welcome to the **Privacy-Preserving Machine Learning Workshop at [EurIPS](http://eurips.cc) 2025**!

The success of machine learning depends on access to large amounts of training data, which often contains sensitive information. This raises issues of legality, competitiveness, and privacy when data is exposed. Neural networks are known to be vulnerable to privacy attacks, a concern that has recently become more visible with large language models (LLMs), where attacks can be carried out directly through prompting. Differential privacy, the gold standard for privacy-preserving learning, has improved in terms of privacy–utility trade-offs thanks to new trust models and algorithms. However, there are still many open questions on how to bridge the gap between attacks and defenses, from developing auditing methods and more effective attacks to the growing interest in machine unlearning.

**Which models best reflect real-world scenarios? How can methods scale to deep learning and foundation models? How are unlearning, auditing, and privacy-preserving machine learning connected, and how can these lines of work be brought together?**

This workshop will bring together researchers from academia and industry working on differential privacy, machine unlearning, privacy auditing, privacy attacks, and related topics.


## Call for Papers {#cfp}

We invite submissions to the **Privacy-Preserving Machine Learning Workshop at EurIPS 2025**.  
We welcome both novel contributions and in-progress work with diverse viewpoints.  


| Important Dates                  | Date (AoE)           |
|-------------------------------------------------|----------------------|
| Paper submission     | **October 10, 2025** |
| Accept/Reject notification            | **October 31, 2025** |
| Workshop                                  | **December 6–7, 2025** |


### Topics of Interest
- Efficient methods for privacy-preserving machine learning  
- Trust models for privacy, including federated learning and data minimization  
- Privacy at inference and privacy for agents interaction and for large language models (fine-tuning, test-time training)  
- Privacy-preserving data generation  
- Differential privacy theory
- Threat models and privacy attacks  
- Auditing methods and interpretation of privacy guarantees  
- Machine unlearning, certifiable machine unlearning, and new unlearning algorithms  
- Relationship between privacy and other issues related to Trustworthy Machine Learning  

### Submission Guidelines
- **Format:** up to **5 pages**, excluding references  
- **Style:** [NeurIPS 2025 template](https://neurips.cc/Conferences/2025/CallForPapers)  
- **Anonymization:** required (double-blind review)  
- **Submission site:** via **OpenReview** — link will be announced here soon  


This workshop is **non-archival and will not have official proceedings**. Workshop submissions can be submitted to other venues. We welcome ongoing and unpublished work, including papers that are under review at the time of submission. We do not accept submissions that have already been accepted for publication in other venues with archival proceedings. The titles of accepted papers will be published on the website.

We are looking for reviewers to help ensure a fair and constructive review process.  
Each reviewer will be asked to review at most **3 papers**.  
If you are interested in serving as a reviewer, **please fill out the following [form](https://forms.gle/o7gtHwc73vpmtK2P9)**.

---
## Invited Speakers {#speakers}

{% include feature_row id="feature_row_bis" %}
and more to be announced

## Program {#program}

**Coming soon!**

---

## Organizers {#organizers}


{% include feature_row id="feature_row" %}



## Contact {#contact}
Questions? Email us at **[ppml.eurips@gmail.com](mailto:ppml.eurips@gmail.com)**
